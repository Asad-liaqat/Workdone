{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f47c20-4451-4de8-9771-b637a7fff713",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ed9357-39b8-463b-bfbd-21510d1edb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import shutil\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72d265-31e3-4d2e-8fa5-f1feaccb682c",
   "metadata": {},
   "source": [
    "## Folder creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee4a119-c7e3-4794-8c08-796565706fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\asad\\practice\\Fish\"\n",
    "listofclass=os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08695d4d-d530-465d-af3e-43fdfa447628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Black Sea Sprat created successfully\n",
      "Directory Gilt-Head Bream created successfully\n",
      "Directory Hourse Mackerel created successfully\n",
      "Directory Red Mullet created successfully\n",
      "Directory Red Sea Bream created successfully\n",
      "Directory Sea Bass created successfully\n",
      "Directory Shrimp created successfully\n",
      "Directory Striped Red Mullet created successfully\n",
      "Directory Trout created successfully\n"
     ]
    }
   ],
   "source": [
    "path1=\"C:/Users/asad/practice/fishdataset/\"\n",
    "directory = \"fishdataset\"\n",
    "\n",
    "# Check if the directory already exists\n",
    "for i in listofclass:\n",
    "    if not os.path.exists(path1+i):\n",
    "        # If not, create it\n",
    "        os.makedirs(path1+i)\n",
    "        print(\"Directory\", i, \"created successfully\")\n",
    "    else:\n",
    "        print(\"Directory\", i, \"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1753e415-248a-4a2a-b39d-4351cee7c307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black Sea Sprat',\n",
       " 'Gilt-Head Bream',\n",
       " 'Hourse Mackerel',\n",
       " 'Red Mullet',\n",
       " 'Red Sea Bream',\n",
       " 'Sea Bass',\n",
       " 'Shrimp',\n",
       " 'Striped Red Mullet',\n",
       " 'Trout']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listofclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61577af-668b-45a5-845a-1f9850e5e195",
   "metadata": {},
   "source": [
    "# Copy from source to destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0674cd73-42bd-4f9e-9532-8043a943049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in listofclass:\n",
    "    name=os.listdir(\"C:/Users/asad/practice/Fish/\"+ i)\n",
    "    image=os.listdir(\"C:/Users/asad/practice/Fish/\"+i+'/'+name[0])\n",
    "    for j in image:\n",
    "        \n",
    "        shutil.copyfile(\"C:/Users/asad/practice/Fish/\"+i+'/'+name[0]+'/'+j,\"C:/Users/asad/practice/fishdataset/\"+name[0]+'/'+j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c158bb06-d4be-466e-b5fb-e240b348a518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 9000 files [01:08, 131.01 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio('fishdataset', output=\"fishdatasettts\", seed=1337, ratio=(.7, 0.2,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687aad25-db7b-4f75-9199-4cf37c2b310f",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ac6c51-7fd8-4ed1-bab7-03d3238691f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = \"C:/Users/asad/practice/CatsAndDogsTTS\"\n",
    "TRAINING_DATA_DIR=\"C:/Users/asad/practice/fishdatasettts/train\"\n",
    "#VALID_DATA_DIR=\"C:/Users/asad/practice/CatsAndDogsTTS/val\"\n",
    "Test_Data_DIR=r\"C:/Users/asad/practice/fishdatasettts/test\"\n",
    "Val_Data_DIR=r\"C:/Users/asad/practice/fishdatasettts/val\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d91192-ddd7-40ce-9dcc-b5caeca04f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (224,224) # (height, width) in no. of pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "881c2f1c-83f8-4e25-ad0e-7808f6978ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6300 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                featurewise_center=False, #set input mean to 0\n",
    "                                                                samplewise_center=False,  #set each sample mean to 0\n",
    "                                                                featurewise_std_normalization=False, #divide input datas to std\n",
    "                                                                samplewise_std_normalization=False,  #divide each datas to own std\n",
    "                                                                zca_whitening=False,  #dimension reduction\n",
    "                                                                rotation_range=0.5,    #rotate 5 degree\n",
    "                                                                zoom_range=0.5,        #zoom in-out 5%\n",
    "                                                                width_shift_range=0.5, #shift 5%\n",
    "                                                                height_shift_range=0.5,\n",
    "                                                                horizontal_flip=True,  #randomly flip images\n",
    "                                                                vertical_flip=True,)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "TRAINING_DATA_DIR,\n",
    "class_mode='categorical',\n",
    "shuffle=True,\n",
    "target_size=IMAGE_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33350a6e-35d7-4244-8967-35d606ae42fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                featurewise_center=False, #set input mean to 0\n",
    "                                                                samplewise_center=False,  #set each sample mean to 0\n",
    "                                                                featurewise_std_normalization=False, #divide input datas to std\n",
    "                                                                samplewise_std_normalization=False,  #divide each datas to own std\n",
    "                                                                zca_whitening=False,  #dimension reduction\n",
    "                                                                rotation_range=0.5,    #rotate 5 degree\n",
    "                                                                zoom_range=0.5,        #zoom in-out 5%\n",
    "                                                                width_shift_range=0.5, #shift 5%\n",
    "                                                                height_shift_range=0.5,\n",
    "                                                                horizontal_flip=True,  #randomly flip images\n",
    "                                                                vertical_flip=True)\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "Test_Data_DIR,\n",
    "class_mode='categorical',\n",
    "shuffle=True,\n",
    "target_size=IMAGE_SHAPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918a024f-f521-4bf6-bb7d-82f4aefafdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                featurewise_center=False, #set input mean to 0\n",
    "                                                                samplewise_center=False,  #set each sample mean to 0\n",
    "                                                                featurewise_std_normalization=False, #divide input datas to std\n",
    "                                                                samplewise_std_normalization=False,  #divide each datas to own std\n",
    "                                                                zca_whitening=False,  #dimension reduction\n",
    "                                                                rotation_range=0.5,    #rotate 5 degree\n",
    "                                                                zoom_range=0.5,        #zoom in-out 5%\n",
    "                                                                width_shift_range=0.5, #shift 5%\n",
    "                                                                height_shift_range=0.5,\n",
    "                                                                horizontal_flip=True,  #randomly flip images\n",
    "                                                                vertical_flip=True)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "Test_Data_DIR,\n",
    "class_mode='categorical',\n",
    "shuffle=True,\n",
    "target_size=IMAGE_SHAPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41a47f12-16a5-4ca5-94a8-9827da751698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Black Sea Sprat': 0, 'Gilt-Head Bream': 1, 'Hourse Mackerel': 2, 'Red Mullet': 3, 'Red Sea Bream': 4, 'Sea Bass': 5, 'Shrimp': 6, 'Striped Red Mullet': 7, 'Trout': 8}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba957d06-1e2a-49f3-b26a-9beb72a62abb",
   "metadata": {},
   "source": [
    "## Architecture Calling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "135da9cb-1163-4d0c-8d6a-07dbe768ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2, MobileNetV2\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e703fe34-65cc-437e-946b-25fa84b2a239",
   "metadata": {},
   "source": [
    "## Kaggle Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d59f82aa-41a3-46c7-b9fb-105df8a11ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D\n",
    "# pre_trained= MobileNetV2(include_top=False, pooling='avg', input_shape=(224,224,3))\n",
    "\n",
    "# #for layers in pre_trained.layers:\n",
    "# #    layers.trainable=False\n",
    "# pre_trained.trainable=False\n",
    "\n",
    "# inp_model = pre_trained.input\n",
    "# #x=Flatten()(pre_trained.output)\n",
    "# x=Dense(128, activation='relu')(pre_trained.output)\n",
    "# x=Dropout(0.5)(x)\n",
    "# x=Dense(128, activation='relu')(x)\n",
    "# output=Dense(9, activation='softmax')(x)\n",
    "# model = Model(inputs=inp_model, outputs=output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b675c9-c55d-43f7-9836-affe644b289b",
   "metadata": {},
   "source": [
    "## Pre-Trained VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bfd38e-4d42-4a18-b8dd-e5011dfbd1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "model=tf.keras.applications.VGG16(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(224,224,3),\n",
    "    pooling=None,\n",
    "    classes=9,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c166e0-f83f-44e9-99bb-911e4b14f40a",
   "metadata": {},
   "source": [
    "## CNN Architecture of Fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "655b7068-eecd-4026-9060-d795c37a9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([\n",
    "#     #1st convo block\n",
    "#     tf.keras.layers.Conv2D(64, (3,3),activation='relu',kernel_initializer='he_uniform', padding='same', input_shape=(224,224,3)),\n",
    "#     tf.keras.layers.Conv2D(64, (3,3),activation='relu', padding='same',kernel_initializer='he_uniform'),\n",
    "#     tf.keras.layers.MaxPool2D(pool_size = (2,2)),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "# #2nd convo bock\n",
    "#     tf.keras.layers.Conv2D(128, (3,3),activation='relu', padding='same',kernel_initializer='he_uniform'),\n",
    "#     tf.keras.layers.MaxPool2D(pool_size = (2,2)),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     #3rd convo block\n",
    "#     tf.keras.layers.Conv2D(256, (3,3),activation='relu', padding='same',kernel_initializer='he_uniform'),\n",
    "#     tf.keras.layers.MaxPool2D(pool_size = (2,2)),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "# #4th conco block\n",
    "#     tf.keras.layers.Conv2D(512, (3,3),activation='relu', padding='same',kernel_initializer='he_uniform'),\n",
    "\n",
    "#     tf.keras.layers.MaxPool2D(pool_size =(2,2)),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     #5th convo block\n",
    "    \n",
    "#     tf.keras.layers.Conv2D(512, (3,3),activation='relu', padding='same',kernel_initializer='he_uniform'),\n",
    "#     tf.keras.layers.MaxPool2D(pool_size =(2,2)),\n",
    "    \n",
    "\n",
    "#     tf.keras.layers.Flatten(),\n",
    "\n",
    "#     # tf.keras.layers.Dense(4096,activation='relu'),\n",
    "#     # # tf.keras.layers.Dropout(0.6),\n",
    "#     # tf.keras.layers.Dense(4096,activation='relu'),\n",
    "#     # # tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "#     tf.keras.layers.Dense(64),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     #For Binary Classes (i.e. Cats&Dogs)\n",
    "#     tf.keras.layers.Dense(9, activation='softmax')\n",
    "#     #For Multi-Class (i.e. Cats&Dogs&Birds....)\n",
    "#     # tf.keras.layers.Dense(3, activation='softmax')\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce94234-06c8-4417-bc29-84d1682adc88",
   "metadata": {},
   "source": [
    "## Optimizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c221bf9d-dc77-44f3-81b0-f32183a6be11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "197/197 [==============================] - 493s 2s/step - loss: 2.1518 - accuracy: 0.1668 - val_loss: 2.0441 - val_accuracy: 0.2200\n",
      "Epoch 2/20\n",
      "197/197 [==============================] - 489s 2s/step - loss: 1.9584 - accuracy: 0.2397 - val_loss: 1.7962 - val_accuracy: 0.3267\n",
      "Epoch 3/20\n",
      "197/197 [==============================] - 827s 4s/step - loss: 1.8700 - accuracy: 0.2654 - val_loss: 1.7197 - val_accuracy: 0.3422\n",
      "Epoch 4/20\n",
      "197/197 [==============================] - 949s 5s/step - loss: 1.8074 - accuracy: 0.2816 - val_loss: 1.7160 - val_accuracy: 0.3456\n",
      "Epoch 5/20\n",
      "197/197 [==============================] - 924s 5s/step - loss: 1.7937 - accuracy: 0.2865 - val_loss: 1.6245 - val_accuracy: 0.3744\n",
      "Epoch 6/20\n",
      "197/197 [==============================] - 606s 3s/step - loss: 1.7685 - accuracy: 0.3040 - val_loss: 1.6521 - val_accuracy: 0.3489\n",
      "Epoch 7/20\n",
      "197/197 [==============================] - 490s 2s/step - loss: 1.7544 - accuracy: 0.3089 - val_loss: 1.6130 - val_accuracy: 0.4178\n",
      "Epoch 8/20\n",
      "197/197 [==============================] - 491s 2s/step - loss: 1.7431 - accuracy: 0.3110 - val_loss: 1.5897 - val_accuracy: 0.4011\n",
      "Epoch 9/20\n",
      "197/197 [==============================] - 502s 3s/step - loss: 1.7288 - accuracy: 0.3130 - val_loss: 1.6019 - val_accuracy: 0.3644\n",
      "Epoch 10/20\n",
      "197/197 [==============================] - 510s 3s/step - loss: 1.7252 - accuracy: 0.3260 - val_loss: 1.5082 - val_accuracy: 0.4144\n",
      "Epoch 11/20\n",
      "197/197 [==============================] - 509s 3s/step - loss: 1.7082 - accuracy: 0.3240 - val_loss: 1.5034 - val_accuracy: 0.4267\n",
      "Epoch 12/20\n",
      "197/197 [==============================] - 513s 3s/step - loss: 1.7103 - accuracy: 0.3348 - val_loss: 1.4724 - val_accuracy: 0.4611\n",
      "Epoch 13/20\n",
      "197/197 [==============================] - 198s 1s/step - loss: 1.6788 - accuracy: 0.3541 - val_loss: 1.4663 - val_accuracy: 0.4556\n",
      "Epoch 14/20\n",
      "197/197 [==============================] - 146s 741ms/step - loss: 1.6939 - accuracy: 0.3427 - val_loss: 1.4970 - val_accuracy: 0.4378\n",
      "Epoch 15/20\n",
      "197/197 [==============================] - 145s 736ms/step - loss: 1.6689 - accuracy: 0.3517 - val_loss: 1.5570 - val_accuracy: 0.3878\n",
      "Epoch 16/20\n",
      "197/197 [==============================] - 146s 739ms/step - loss: 1.6940 - accuracy: 0.3424 - val_loss: 1.5086 - val_accuracy: 0.4322\n",
      "Epoch 17/20\n",
      "197/197 [==============================] - 145s 736ms/step - loss: 1.6686 - accuracy: 0.3522 - val_loss: 1.5055 - val_accuracy: 0.4267\n",
      "Epoch 18/20\n",
      "197/197 [==============================] - 144s 733ms/step - loss: 1.6603 - accuracy: 0.3617 - val_loss: 1.4424 - val_accuracy: 0.4767\n",
      "Epoch 19/20\n",
      "197/197 [==============================] - 148s 751ms/step - loss: 1.6785 - accuracy: 0.3567 - val_loss: 1.4960 - val_accuracy: 0.4289\n",
      "Epoch 20/20\n",
      "197/197 [==============================] - 160s 812ms/step - loss: 1.6387 - accuracy: 0.3681 - val_loss: 1.4412 - val_accuracy: 0.4767\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# optimizer = \"Adam\"\n",
    "# model.compile(\n",
    "#  optimizer=optimizer,\n",
    "#  loss=\"categorical_crossentropy\",\n",
    "#  #For MultiClass use categorical_crossentropy\n",
    "#  #For Binary Class use binary_crossentropy\n",
    "\n",
    "#  metrics=[\"accuracy\"])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=8)\n",
    "\n",
    "history = model.fit(train_generator, validation_data=valid_generator, epochs=20, callbacks=callback).history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4728f10-b5a5-4d97-96f2-0c08f0d6aa37",
   "metadata": {},
   "source": [
    "## Prediction of New Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5488b7c4-8c87-427c-86a0-f993c7f8286e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[[3.7029266e-31 0.0000000e+00 0.0000000e+00 2.9527148e-16 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+00 6.4208284e-29 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# make a prediction for a new image.\n",
    "from tensorflow.keras.utils import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "\t# load the image\n",
    "\timg = load_img(filename, target_size=(224,224))\n",
    "\t# convert to array\n",
    "\timg = img_to_array(img)\n",
    "\t# reshape into a single sample with 3 channels\n",
    "\timg = img.reshape(1, 224,224, 3)\n",
    "\t# center pixel data\n",
    "\timg = img.astype('float32')\n",
    "\t#img = img - [123.68, 116.779, 103.939]\n",
    "\treturn img\n",
    "\n",
    "# load an image and predict the class\n",
    "def run_example():\n",
    "\t# load the image\n",
    "\timg = load_image('C:/Users/asad/practice/images.png')\n",
    "\t# load model\n",
    "\t#model = load_model('final_model.h5')\n",
    "\t# predict the class\n",
    "\tresult = model.predict(img)\n",
    "\tprint(result)\n",
    "\n",
    "\n",
    "# entry point, run the example\n",
    "run_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed404c-3b43-42ed-b71e-fe45170383b5",
   "metadata": {},
   "source": [
    "## Evaluating Model for New Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c183565-1fdb-4bb2-91c3-02e8b01034b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 18s 596ms/step - loss: 1.4494 - accuracy: 0.4556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4494236707687378, 0.4555555582046509]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169bf3cf-48ff-4ba5-9b84-a4b67444a1af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
